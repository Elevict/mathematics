---
tags:
  - mathematics/ordinary_differential_equations
---
# Review of Matrices

## Matrices and Their Properties

We designate matrices by boldfaced capitals $\mathbf{A}, \mathbf{B}, \mathbf{C}, \ldots$, occasionally using boldfaced Greek capitals $\mathbf{\Phi}, \mathbf{\Psi}, \ldots$. A matrix $\mathbf{A}$ consists of a rectangular array of numbers, or elements, arranged in $m$ rows and $n$ columnsâ€”that is,

$$
\mathbf{A} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}.
$$

We speak of $\mathbf{A}$ as an $m \times n$ matrix. The element lying in the $i$th row and $j$th column is designated by $a_{ij}$, where the first subscript identifies its row and the second its column. Sometimes, the notation $(a_{ij})$ is used to denote the matrix whose generic element is $a_{ij}$.

### Special Matrices

- **Transpose:** The transpose of $\mathbf{A}$, denoted $\mathbf{A}^T$, is obtained by interchanging the rows and columns of $\mathbf{A}$. If $\mathbf{A} = (a_{ij})$, then $\mathbf{A}^T = (a_{ji})$.
- **Conjugate:** Denoted $\overline{\mathbf{A}}$, is obtained by replacing each element $a_{ij}$ with its complex conjugate $\overline{a_{ij}}$.
- **Adjoint:** The adjoint of $\mathbf{A}$, denoted $\mathbf{A}^*$, is the transpose of the conjugate matrix: $\mathbf{A}^* = \overline{\mathbf{A}}^T$.

For example, if

$$
\mathbf{A} =
\begin{pmatrix}
3 & 2 - i \\
4 + 3i & -5 + 2i
\end{pmatrix},
$$

then:

$$
\mathbf{A}^T =
\begin{pmatrix}
3 & 4 + 3i \\
2 - i & -5 + 2i
\end{pmatrix}, \quad
\overline{\mathbf{A}} =
\begin{pmatrix}
3 & 2 + i \\
4 - 3i & -5 - 2i
\end{pmatrix}, \quad
\mathbf{A}^* =
\begin{pmatrix}
3 & 4 - 3i \\
2 + i & -5 - 2i
\end{pmatrix}.
$$

### Types of Matrices

1. **Square Matrices:** Matrices with the same number of rows and columns ($m = n$). Such matrices are said to be of order $n$.
2. **Vectors:** Column vectors can be thought of as $n \times 1$ matrices. The transpose of a column vector is a $1 \times n$ row vector.

## Matrix Operations

### 1. Equality

Two $m \times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ are equal if $a_{ij} = b_{ij}$ for all $i$ and $j$.

### 2. Zero Matrix

The symbol $\mathbf{0}$ denotes the matrix (or vector) whose elements are all zero.

### 3. Addition

The sum of two $m \times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ is given by:

$$
\mathbf{A} + \mathbf{B} = (a_{ij}) + (b_{ij}) = (a_{ij} + b_{ij}).
$$

Matrix addition is commutative and associative:

$$
\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}, \quad \mathbf{A} + (\mathbf{B} + \mathbf{C}) = (\mathbf{A} + \mathbf{B}) + \mathbf{C}.
$$

### 4. Scalar Multiplication

The product of a matrix $\mathbf{A}$ by a scalar $\alpha$ is:

$$
\alpha \mathbf{A} = \alpha (a_{ij}) = (\alpha a_{ij}).
$$

This operation satisfies distributive laws:

$$
\alpha(\mathbf{A} + \mathbf{B}) = \alpha\mathbf{A} + \alpha\mathbf{B}, \quad (\alpha + \beta)\mathbf{A} = \alpha\mathbf{A} + \beta\mathbf{A}.
$$

### 5. Subtraction

The difference of two $m \times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ is defined as:

$$
\mathbf{A} - \mathbf{B} = \mathbf{A} + (-\mathbf{B}),
$$

where $-\mathbf{B} = (-1)\mathbf{B}$.

### 6. Matrix Multiplication

The product $\mathbf{AB}$ is defined if the number of columns of $\mathbf{A}$ equals the number of rows of $\mathbf{B}$. If $\mathbf{A}$ is $m \times n$ and $\mathbf{B}$ is $n \times r$, then $\mathbf{C} = \mathbf{AB}$ is an $m \times r$ matrix, with elements:

$$
c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
$$

Matrix multiplication satisfies:
1. Associativity: $(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})$.
2. Distributivity: $\mathbf{A}(\mathbf{B} + \mathbf{C}) = \mathbf{AB} + \mathbf{AC}$.

However, matrix multiplication is not generally commutative:

$$
\mathbf{AB} \neq \mathbf{BA}.
$$

### 7. Identity Matrix

The identity matrix $\mathbf{I}$ of order $n$ is a square matrix with ones on the diagonal and zeros elsewhere:

$$
\mathbf{I} =
\begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}.
$$

For any square matrix $\mathbf{A}$, we have:

$$
\mathbf{AI} = \mathbf{IA} = \mathbf{A}.
$$

## Examples

### Example 1: Non-commutative Multiplication

Let:

$$
\mathbf{A} =
\begin{pmatrix}
1 & -2 & 1 \\
0 & 2 & -1 \\
2 & 1 & 1
\end{pmatrix}, \quad
\mathbf{B} =
\begin{pmatrix}
2 & 1 & -1 \\
1 & -1 & 0 \\
2 & -1 & 1
\end{pmatrix}.
$$

Compute:

$$
\mathbf{AB} =
\begin{pmatrix}
2 & 2 & 0 \\
0 & -1 & -1 \\
7 & 0 & -1
\end{pmatrix}, \quad
\mathbf{BA} =
\begin{pmatrix}
0 & -3 & 0 \\
1 & -4 & 2 \\
4 & -5 & 4
\end{pmatrix}.
$$

Clearly, $\mathbf{AB} \neq \mathbf{BA}$.
