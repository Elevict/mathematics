---
tags:
  - mathematics/ordinary_differential_equations
---
# Basic Theory of Systems of First order Linear Equations

The general theory of a system of $n$ first-order linear equations

$$
x'_1 = p_{11}(t)x_1 + \cdots + p_{1n}(t)x_n + g_1(t), \dots, \quad x'_n = p_{n1}(t)x_1 + \cdots + p_{nn}(t)x_n + g_n(t) \tag{1}
$$

closely parallels that of a single linear equation of $n$th order. To discuss the system effectively, we write it in matrix notation. That is, we consider $x_1 = \varphi_1(t), \dots, x_n = \varphi_n(t)$ to be components of a vector $x = \varphi(t)$; similarly, $g_1(t), \dots, g_n(t)$ are components of a vector $g(t)$, and $p_{11}(t), \dots, p_{nn}(t)$ are elements of an $n \times n$ matrix $P(t)$. Equation (1) then takes the form:

$$
x' = P(t)x + g(t) \tag{2}
$$

The use of vectors and matrices not only saves a great deal of space and facilitates calculations but also emphasizes the similarity between systems of equations and single (scalar) equations.

A vector $x = \varphi(t)$ is said to be a solution of Eq. (2) if its components satisfy the system of equations (1). Throughout this section, we assume that $P$ and $g$ are continuous on some interval $\alpha < t < \beta$; that is, each of the scalar functions $p_{11}, \dots, p_{nn}, g_1, \dots, g_n$ is continuous there. According to Theorem 7.1.2, this is sufficient to guarantee the existence of solutions of Eq. (2) on the interval $\alpha < t < \beta$.

We first consider the homogeneous equation

$$
x' = P(t)x \tag{3}
$$

obtained from Eq. (2) by setting $g(t) = 0$. Once the homogeneous equation has been solved, several methods can be used to solve the nonhomogeneous equation (2), which is taken up in Section 7.9. We use the notation

$$
x^{(1)}(t) = \begin{pmatrix} x_{11}(t) \\ x_{21}(t) \\ \vdots \\ x_{n1}(t) \end{pmatrix}, \dots, x^{(k)}(t) = \begin{pmatrix} x_{1k}(t) \\ x_{2k}(t) \\ \vdots \\ x_{nk}(t) \end{pmatrix}, \dots \tag{4}
$$

to designate specific solutions of the system (3). Note that $x_{ij}(t) = x^{(j)}_i(t)$ refers to the $i$th component of the $j$th solution $x^{(j)}(t)$. The main facts about the structure of solutions of the system (3) are stated in Theorems 7.4.1 to 7.4.5.

> [!theorem] **Theorem 7.4.1** (Superposition Principle)
> If the vector functions $x^{(1)}$ and $x^{(2)}$ are solutions of the system (3), then the linear combination $c_1x^{(1)} + c_2x^{(2)}$ is also a solution for any constants $c_1$ and $c_2$.

*Proof*
This is the principle of superposition. It is proved by differentiating $c_1x^{(1)} + c_2x^{(2)}$ and using the fact that $x^{(1)}$ and $x^{(2)}$ satisfy Eq. (3). As an example, it can be verified that

$$
x^{(1)}(t) = \begin{pmatrix} e^{3t} \\ 2e^{3t} \end{pmatrix} = \begin{pmatrix} 1 \\ 2 \end{pmatrix} e^{3t}, \quad x^{(2)}(t) = \begin{pmatrix} e^{-t} \\ -2e^{-t} \end{pmatrix} = \begin{pmatrix} 1 \\ -2 \end{pmatrix} e^{-t}
$$

satisfy the equation

$$
x' = \begin{pmatrix} 1 & 1 \\ 4 & 1 \end{pmatrix} x \tag{5}
$$

Then, according to Theorem 7.4.1,

$$
x = c_1 \begin{pmatrix} 1 \\ 2 \end{pmatrix} e^{3t} + c_2 \begin{pmatrix} 1 \\ -2 \end{pmatrix} e^{-t} \tag{6}
$$

also satisfies Eq. (6). By repeated application of Theorem 7.4.1, we can conclude that if $x^{(1)}, \dots, x^{(k)}$ are solutions of Eq. (3), then

$$
x = c_1 x^{(1)}(t) + \cdots + c_k x^{(k)}(t) \tag{7}
$$

is also a solution for any constants $c_1, \dots, c_k$. Thus, every finite linear combination of solutions of Eq. (3) is also a solution.

$$
\tag*{\(\blacksquare\)}
$$

> [!theorem] **Theorem 7.4.2**
If the vector functions $x^{(1)}, \dots, x^{(n)}$ are linearly independent solutions of the system (3) for each point in the interval $\alpha < t < \beta$, then each solution $x = \varphi(t)$ of the system (3) can be expressed as a linear combination of $x^{(1)}, \dots, x^{(n)}$ in exactly one way:

$$
\varphi(t) = c_1 x^{(1)}(t) + \cdots + c_n x^{(n)}(t) \tag{8}
$$

*Proof*:
Theorem 7.4.1 ensures that all expressions of the form above are solutions of the system (3), while Theorem 7.4.2 guarantees that all solutions of Eq. (3) can be written in this form. If the constants $c_1, \dots, c_n$ are arbitrary, then Eq. (11) includes all solutions of the system (3), and it is customary to call it the general solution. Any set of solutions $x^{(1)}, \dots, x^{(n)}$ of Eq. (3) that is linearly independent at each point in the interval $\alpha < t < \beta$ is said to be a fundamental set of solutions for that interval.

$$
\tag*{\(\blacksquare\)}
$$

> [!theorem] **Theorem 7.4.3**
If $x^{(1)}, \dots, x^{(n)}$ are solutions of Eq. (3) on the interval $\alpha < t < \beta$, then in this interval, $W[x^{(1)}, \dots, x^{(n)}]$ either is identically zero or else never vanishes.

*Proof*:
The Wronskian $W[x^{(1)}, \dots, x^{(n)}]$ satisfies the differential equation:

$$
\frac{dW}{dt} = [p_{11}(t) + p_{22}(t) + \cdots + p_{nn}(t)] W \tag{9}
$$

Hence,

$$
W(t) = c \exp \left( \int [p_{11}(t) + \cdots + p_{nn}(t)] dt \right) \tag{10}
$$

where $c$ is an arbitrary constant. The conclusion of the theorem follows immediately from this expression. This result is known as Abel's formula.

$$
\tag*{\(\blacksquare\)}
$$

> [!theorem] **Theorem 7.4.4**
> Let
>
> $$
> e^{(1)} = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \quad e^{(2)} = \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix}, \quad \dots, \quad e^{(n)} = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix} \tag{11}
> $$
>
> and let $x^{(1)}, \dots, x^{(n)}$ be the solutions of the system (3) that satisfy the initial conditions
>
> $$
> x^{(1)}(t_0) = e^{(1)}, \dots, x^{(n)}(t_0) = e^{(n)}, \tag{12}
> $$
>
> respectively, where $t_0$ is any point in $\alpha < t < \beta$. Then $x^{(1)}, \dots, x^{(n)}$ form a fundamental set of solutions of the system (3).

*Proof*:
The existence and uniqueness of the solutions $x^{(1)}, \dots, x^{(n)}$ are ensured by Theorem 7.1.2. It is clear that the Wronskian of these solutions is equal to 1 when $t = t_0$, and therefore $x^{(1)}, \dots, x^{(n)}$ form a fundamental set of solutions.

$$
\tag*{\(\blacksquare\)}
$$

> [!theorem] **Theorem 7.4.5**
> Consider the system
>
> $$
> x' = P(t)x, \tag{13}
> $$
>
> where each element of $P$ is a real-valued continuous function. If $x = u(t) + iv(t)$ is a complex-valued solution of Eq. (3), then its real part $u(t)$ and its imaginary part $v(t)$ are also solutions of this equation.

*Proof*:
Substitute $u(t) + iv(t)$ into the system. Differentiating both sides with respect to $t$ gives

$$
u'(t) + iv'(t) = P(t)(u(t) + iv(t)), \tag{14}
$$

which separates into the real and imaginary parts:

$$
u'(t) = P(t)u(t), \quad v'(t) = P(t)v(t). \tag{15}
$$

Thus, $u(t)$ and $v(t)$ are solutions of the system.

$$
\tag*{\(\blacksquare\)}
$$

In conclusion, the fundamental solutions of the system $x' = P(t)x$ provide the necessary framework for solving systems of first-order linear differential equations. The linearity of the system and the superposition principle allow for the construction of general solutions from a set of fundamental solutions. These theoretical results provide powerful tools for analyzing and solving such systems.